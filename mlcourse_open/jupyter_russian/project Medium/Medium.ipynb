{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from html.parser import HTMLParser\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "def read_json_line(line=None):\n",
    "    result = None\n",
    "    try:\n",
    "        result = json.loads(line)\n",
    "    except Exception as e:\n",
    "        # Find the offending character index:\n",
    "        idx_to_replace = int(str(e).split(' ')[-1].replace(')',''))\n",
    "        # Remove the offending character:\n",
    "        new_line = list(line)\n",
    "        new_line[idx_to_replace] = ' '\n",
    "        new_line = ''.join(new_line)\n",
    "        return read_json_line(line=new_line)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ПОЛЕ       ПОЛЕЗНОСТЬ\n",
    "# ids        да\n",
    "# spider     полность нет, 1 значение\n",
    "# timestamp  наверное\n",
    "# author     да\n",
    "# content    да\n",
    "# domain     да\n",
    "# image_url  наверно\n",
    "# link_tags  наверно\n",
    "# meta_tages наверно\n",
    "# published  да\n",
    "# tags       полностью нет, 1 пустое значение\n",
    "# title      да\n",
    "# url        наверно\n",
    "\n",
    "\n",
    "\n",
    "def load_ids(path_to_inp_json_file):\n",
    "    output_list = list()\n",
    "    with open(path_to_inp_json_file) as inp_file:\n",
    "        for line in tqdm(inp_file):\n",
    "            json_data = read_json_line(line)\n",
    "            id = json_data['_id']\n",
    "            output_list.append(id)\n",
    "\n",
    "    return pd.DataFrame({'id': output_list})\n",
    "\n",
    "def load_timestamps(path_to_inp_json_file):\n",
    "    output_list = list()\n",
    "    with open(path_to_inp_json_file) as inp_file:\n",
    "        for line in tqdm(inp_file):\n",
    "            json_data = read_json_line(line)\n",
    "            timestamp = json_data['_timestamp']\n",
    "            output_list.append(timestamp)\n",
    "\n",
    "    return pd.DataFrame({'timestamp': output_list})\n",
    "\n",
    "def load_authors(path_to_inp_json_file):\n",
    "    output_list = list()\n",
    "    with open(path_to_inp_json_file) as inp_file:\n",
    "        for line in tqdm(inp_file):\n",
    "            json_data = read_json_line(line)\n",
    "            author = json_data['author']\n",
    "            output_list.append(author)\n",
    "\n",
    "    # name\n",
    "    return pd.DataFrame.from_dict(output_list)[['twitter', 'url']]\n",
    "\n",
    "def load_contents(path_to_inp_json_file):\n",
    "    output_list = []\n",
    "    with open(path_to_inp_json_file) as inp_file:\n",
    "        for line in tqdm(inp_file):\n",
    "            json_data = read_json_line(line)\n",
    "            content = json_data['content'].replace('\\n', ' ').replace('\\r', ' ')\n",
    "            content_no_html_tags = strip_tags(content)\n",
    "            output_list.append(content_no_html_tags)\n",
    "    return pd.DataFrame(output_list)\n",
    "\n",
    "def load_domains(path_to_inp_json_file):\n",
    "    output_list = list()\n",
    "    with open(path_to_inp_json_file) as inp_file:\n",
    "        for line in tqdm(inp_file):\n",
    "            json_data = read_json_line(line)\n",
    "            domain = json_data['domain']\n",
    "            output_list.append(domain)\n",
    "\n",
    "    return pd.DataFrame({'domain': output_list})\n",
    "\n",
    "def load_image_urls(path_to_inp_json_file):\n",
    "    output_list = list()\n",
    "    with open(path_to_inp_json_file) as inp_file:\n",
    "        for line in tqdm(inp_file):\n",
    "            json_data = read_json_line(line)\n",
    "            image_url = json_data['image_url']\n",
    "            output_list.append(image_url)\n",
    "\n",
    "    return pd.DataFrame({'image_url': output_list})\n",
    "\n",
    "def load_link_tags(path_to_inp_json_file):\n",
    "    output_list = list()\n",
    "    with open(path_to_inp_json_file) as inp_file:\n",
    "        for line in tqdm(inp_file):\n",
    "            json_data = read_json_line(line)\n",
    "            link_tags = json_data['link_tags']\n",
    "            output_list.append(link_tags)\n",
    "\n",
    "    #drop icon, mask-icon, publisher, search\n",
    "    return pd.DataFrame.from_dict(output_list)[['alternate', 'amphtml', 'apple-touch-icon', 'author', 'canonical', 'stylesheet']]\n",
    "\n",
    "def load_meta_tags(path_to_inp_json_file):\n",
    "    output_list = list()\n",
    "    with open(path_to_inp_json_file) as inp_file:\n",
    "        for line in tqdm(inp_file):\n",
    "            json_data = read_json_line(line)\n",
    "            meta_tags = json_data['meta_tags']\n",
    "            output_list.append(meta_tags)\n",
    "\n",
    "    useless_columns = ['al:android:app_name', 'al:android:package', 'al:ios:app_name', 'al:ios:app_store_id',\n",
    "                       'fb:app_id', 'og:type', 'theme-color', 'twitter:app:id:iphone', 'twitter:app:name:iphone',\n",
    "                       'twitter:label1', 'viewport']\n",
    "    output_df = pd.DataFrame.from_dict(output_list)\n",
    "    return output_df[output_df.columns.difference(useless_columns)]\n",
    "\n",
    "def load_publisheds(path_to_inp_json_file):\n",
    "    output_list = list()\n",
    "    with open(path_to_inp_json_file) as inp_file:\n",
    "        for line in tqdm(inp_file):\n",
    "            json_data = read_json_line(line)\n",
    "            published = json_data['published']\n",
    "            output_list.append(published['$date'])\n",
    "\n",
    "    return pd.DataFrame({'published_$date': output_list})\n",
    "\n",
    "def load_titles(path_to_inp_json_file):\n",
    "    output_list = list()\n",
    "    with open(path_to_inp_json_file) as inp_file:\n",
    "        for line in tqdm(inp_file):\n",
    "            json_data = read_json_line(line)\n",
    "            title = json_data['title']\n",
    "            output_list.append(title)\n",
    "\n",
    "    return pd.DataFrame({'title': output_list})\n",
    "\n",
    "def load_urls(path_to_inp_json_file):\n",
    "    output_list = list()\n",
    "    with open(path_to_inp_json_file) as inp_file:\n",
    "        for line in tqdm(inp_file):\n",
    "            json_data = read_json_line(line)\n",
    "            url = json_data['url']\n",
    "            output_list.append(url)\n",
    "\n",
    "    return pd.DataFrame({'url': output_list})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34645it [01:44, 333.09it/s]\n",
      "62313it [02:58, 348.33it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "\n",
    "PATH_TO_DATA = './'\n",
    "\n",
    "def read_json_line(line=None):\n",
    "    result = None\n",
    "    try:\n",
    "        result = json.loads(line)\n",
    "    except Exception as e:\n",
    "        # Find the offending character index:\n",
    "        idx_to_replace = int(str(e).split(' ')[-1].replace(')',''))\n",
    "        # Remove the offending character:\n",
    "        new_line = list(line)\n",
    "        new_line[idx_to_replace] = ' '\n",
    "        new_line = ''.join(new_line)\n",
    "        return read_json_line(line=new_line)\n",
    "    return result\n",
    "\n",
    "def preprocess(path_to_inp_json_file):\n",
    "    output_list = []\n",
    "    with open(path_to_inp_json_file) as inp_file:\n",
    "        for line in tqdm(inp_file):\n",
    "            json_data = read_json_line(line)\n",
    "            content = json_data['content'].replace('\\n', ' ').replace('\\r', ' ')\n",
    "            content_no_html_tags = strip_tags(content)\n",
    "            output_list.append(content_no_html_tags)\n",
    "    return output_list\n",
    "\n",
    "test_raw_content = preprocess(os.path.join(PATH_TO_DATA, 'test.json'),)\n",
    "train_raw_content = preprocess(os.path.join(PATH_TO_DATA, 'train.json'),)\n",
    "\n",
    "\n",
    "#cv = CountVectorizer(max_features=100000)\n",
    "#X_train = cv.fit_transform(train_raw_content)\n",
    "#X_test = cv.transform(test_raw_content)\n",
    "\n",
    "train_target = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_log1p_recommends.csv'), index_col='id')\n",
    "y_train = train_target['log_recommends'].values\n",
    "\n",
    "#ridge = Ridge(random_state=17)\n",
    "\n",
    "\n",
    "## HOLDOUT\n",
    "# train_part_size = int(0.7 * train_target.shape[0])\n",
    "# X_train_part = X_train[:train_part_size, :]\n",
    "# y_train_part = y_train[:train_part_size]\n",
    "# X_valid =  X_train[train_part_size:, :]\n",
    "# y_valid = y_train[train_part_size:]\n",
    "#\n",
    "# ridge.fit(X_train_part, y_train_part);\n",
    "# ridge_pred = ridge.predict(X_valid)\n",
    "# valid_mae = mean_absolute_error(y_valid, ridge_pred)\n",
    "# valid_mae, np.expm1(valid_mae)\n",
    "#\n",
    "# ridge.fit(X_train, y_train);\n",
    "\n",
    "\n",
    "\n",
    "#ridge.fit(X_train, y_train);\n",
    "#ridge_test_pred = ridge.predict(X_test)\n",
    "\n",
    "\n",
    "def write_submission_file(prediction, filename,\n",
    "                          path_to_sample=os.path.join(PATH_TO_DATA, 'sample_submission.csv')):\n",
    "    submission = pd.read_csv(path_to_sample, index_col='id')\n",
    "\n",
    "    submission['log_recommends'] = prediction\n",
    "    submission.to_csv(filename)\n",
    "\n",
    "#write_submission_file(prediction=ridge_test_pred, filename='first_ridge.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/danil/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# подключим необходимые библиотеки\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import pymorphy2\n",
    "from gensim.models import word2vec\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stops = set(stopwords.words(\"english\")) | set(stopwords.words(\"russian\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_wordlist(review):\n",
    "    #1)\n",
    "    review_text = re.sub(\"[^а-яА-Яa-zA-Z]\",\" \", review)\n",
    "    #2)\n",
    "    words = review_text.lower().split()\n",
    "    #3)\n",
    "    words = [w for w in words if not w in stops]\n",
    "    #4)\n",
    "    #words = [morph.parse(w)[0].normal_form for w in words ]\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62313it [00:19, 3205.67it/s]\n",
      "34645it [00:10, 3277.43it/s]\n"
     ]
    }
   ],
   "source": [
    "other_data_train = pd.DataFrame()\n",
    "other_data_test = pd.DataFrame()\n",
    "other_data_train['published']=load_publisheds(os.path.join(PATH_TO_DATA, 'train.json'))\n",
    "other_data_test['published']=load_publisheds(os.path.join(PATH_TO_DATA, 'test.json'))\n",
    "\n",
    "# Преобразуем время\n",
    "other_data_train['published'] = other_data_train['published'].apply(pd.to_datetime)\n",
    "other_data_train['year'] = other_data_train['published'].apply(lambda x: x.year)\n",
    "other_data_train['month'] = other_data_train['published'].apply(lambda x: x.month)\n",
    "\n",
    "other_data_test['published'] = other_data_test['published'].apply(pd.to_datetime)\n",
    "other_data_test['year'] = other_data_test['published'].apply(lambda x: x.year)\n",
    "other_data_test['month'] = other_data_test['published'].apply(lambda x: x.month)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.DataFrame(train_raw_content,columns=['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.1 s, sys: 1 s, total: 40.1 s\n",
      "Wall time: 40.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train['content'] = train['content'].apply(review_to_wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22min 7s, sys: 2.77 s, total: 22min 10s\n",
      "Wall time: 5min 52s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = word2vec.Word2Vec(train['content'], size=300, window=10, workers=4)\n",
    "w2v = dict(zip(model.wv.index2word, model.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'also', 0.543807327747345),\n",
       " (u'using', 0.49845319986343384),\n",
       " (u'work', 0.49001190066337585),\n",
       " (u'use', 0.48959293961524963),\n",
       " (u'better', 0.4888975918292999),\n",
       " (u'great', 0.486032098531723),\n",
       " (u'research', 0.47841358184814453),\n",
       " (u'technology', 0.4764072000980377),\n",
       " (u'one', 0.4740687608718872),\n",
       " (u'matterif', 0.4722328782081604)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Посмотрим чему выучилась модель:\n",
    "model.wv.most_similar(positive=['open', 'data','science','best'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#пропишем класс выполняющий tfidf преобразование.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "\n",
    "class tfidf_vectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = len(next(iter(w2v.values())))\n",
    "\n",
    "    def fit(self, X):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = tfidf_vectorizer(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 57min 34s, sys: 1.81 s, total: 1h 57min 36s\n",
      "Wall time: 1h 57min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train=tfidf.fit(train['content']).transform(train['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split(train,y,ratio):\n",
    "    idx = ratio\n",
    "    return train[:idx, :], train[idx:, :], y[:idx], y[idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40000, 300), (22313, 300), 3.0935355707499999, 2.9762515950342849)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr, Xval, ytr, yval = split(train, y_train, 40000)\n",
    "Xtr.shape,Xval.shape,ytr.mean(),yval.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'median'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-7f831d3d00bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mridge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvalid_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mridge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mymed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mytr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Ошибка на трейне'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mytr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Ошибка на валидации'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'median'"
     ]
    }
   ],
   "source": [
    "ridge = Ridge(alpha = 1,random_state=7)\n",
    "ridge.fit(Xtr, ytr)\n",
    "train_preds = ridge.predict(Xtr)\n",
    "valid_preds = ridge.predict(Xval)\n",
    "print('Ошибка на трейне',mean_squared_error(ytr, train_preds))\n",
    "print('Ошибка на валидации',mean_squared_error(yval, valid_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\xd0\\x9e\\xd1\\x88\\xd0\\xb8\\xd0\\xb1\\xd0\\xba\\xd0\\xb0 \\xd0\\xbd\\xd0\\xb0 \\xd1\\x82\\xd1\\x80\\xd0\\xb5\\xd0\\xb9\\xd0\\xbd\\xd0\\xb5', 1.3612604571809916)\n",
      "('\\xd0\\x9e\\xd1\\x88\\xd0\\xb8\\xd0\\xb1\\xd0\\xba\\xd0\\xb0 \\xd0\\xbd\\xd0\\xb0 \\xd0\\xb2\\xd0\\xb0\\xd0\\xbb\\xd0\\xb8\\xd0\\xb4\\xd0\\xb0\\xd1\\x86\\xd0\\xb8\\xd0\\xb8', 1.3071529573134417)\n"
     ]
    }
   ],
   "source": [
    "print('Ошибка на трейне',mean_absolute_error(ytr, train_preds))\n",
    "print('Ошибка на валидации',mean_absolute_error(yval, valid_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обучим на всей выборке**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ridge.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34645it [01:49, 316.47it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test = pd.DataFrame(test_raw_content,columns=['content'])\n",
    "X_test['content'] = X_test['content'].apply(review_to_wordlist)\n",
    "X_test = tfidf.transform(X_test['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ridge_test_pred = ridge.predict(X_test)\n",
    "write_submission_file(prediction=ridge_test_pred, filename='ridge_with_wor2vec_and_tfidf.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Попробуем нейронные сети.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# подключим библиотеки keras \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import regularizers\n",
    "from keras.wrappers.scikit_learn import KerasRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Опишем нашу сеть.\n",
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=Xtr.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "estimator = KerasRegressor(build_fn=baseline_model,epochs=20, nb_epoch=20, batch_size=64,validation_data=(Xval, yval), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator.fit(Xtr, ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34645it [00:11, 3097.98it/s]\n",
      "34645it [00:12, 2774.77it/s]\n",
      "34645it [00:11, 3100.38it/s]\n",
      "34645it [00:10, 3161.17it/s]\n",
      "34645it [00:10, 3154.94it/s]\n",
      "34645it [00:11, 2962.71it/s]\n",
      "34645it [00:11, 3037.47it/s]\n",
      "34645it [00:11, 3105.80it/s]\n"
     ]
    }
   ],
   "source": [
    "other_data=pd.DataFrame()\n",
    "other_data['timestamp']=load_timestamps(os.path.join(PATH_TO_DATA, 'test.json'))\n",
    "other_data['author_twitter']=load_authors(os.path.join(PATH_TO_DATA, 'test.json'))['twitter']\n",
    "other_data['author_url']=load_authors(os.path.join(PATH_TO_DATA, 'test.json'))['url']\n",
    "other_data['domain']=load_domains(os.path.join(PATH_TO_DATA, 'test.json'))\n",
    "other_data['published']=load_publisheds(os.path.join(PATH_TO_DATA, 'test.json'))\n",
    "other_data['title']=load_titles(os.path.join(PATH_TO_DATA, 'test.json'))\n",
    "other_data['image_url']=load_image_urls(os.path.join(PATH_TO_DATA, 'test.json'))\n",
    "#other_data['meta_tag']=load_meta_tags(os.path.join(PATH_TO_DATA, 'test.json'))\n",
    "#other_data['link_tags']=load_link_tags(os.path.join(PATH_TO_DATA, 'test.json'))\n",
    "other_data['url']=load_urls(os.path.join(PATH_TO_DATA, 'test.json'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "other_data_train = pd.DataFrame()\n",
    "other_data_train['title'] = load_titles(os.path.join(PATH_TO_DATA, 'train.json'))\n",
    "#other_data_train['published']=load_publisheds(os.path.join(PATH_TO_DATA, 'train.json'))\n",
    "#other_data_train['domain']=load_domains(os.path.join(PATH_TO_DATA, 'train.json'))\n",
    "#other_data_train['domain']=other_data_train['domain'].str.split('.').str.get(-1)\n",
    "#le_domen=LabelEncoder()\n",
    "#other_data_train['domain']=le_domen.fit_transform(other_data_train['domain'])\n",
    "\n",
    "\n",
    "#published_year_month_scaler=StandardScaler()\n",
    "#other_data_train['published_year_month']=pd.DatetimeIndex(other_data_train['published']).year*100+pd.DatetimeIndex(other_data_train['published']).month\n",
    "#other_data_train['published_year_month']=published_year_month_scaler.fit_transform(other_data_train['published_year_month'].as_matrix().reshape(-1,1))\n",
    "\n",
    "other_data_test = pd.DataFrame()\n",
    "other_data_test['title'] = load_titles(os.path.join(PATH_TO_DATA, 'test.json'))\n",
    "#other_data_test['published']=load_publisheds(os.path.join(PATH_TO_DATA, 'test.json'))\n",
    "#other_data_test['published_year_month']=pd.DatetimeIndex(other_data_test['published']).year*100+pd.DatetimeIndex(other_data_test['published']).month\n",
    "#other_data_test['published_year_month']=published_year_month_scaler.transform(other_data_test['published_year_month'].as_matrix().reshape(-1,1))\n",
    "#other_data_test['domain']=load_domains(os.path.join(PATH_TO_DATA, 'test.json'))\n",
    "#other_data_test['domain']=other_data_test['domain'].str.split('.').str.get(-1)\n",
    "#other_data_test['domain']=le_domen.transform(other_data_test['domain'])\n",
    "\n",
    "CountVectorizer_title = CountVectorizer()\n",
    "other_data_train_title = CountVectorizer_title.fit_transform(other_data_train['title'])\n",
    "other_data_test_title = CountVectorizer_title.transform(other_data_test['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "no supported conversion for types: (dtype('int64'), dtype('O'), dtype('int64'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-6805296979b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mother_data_train_title\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mother_data_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'domain'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mother_data_test_title\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mother_data_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'domain'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mridge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mridge_test_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mridge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mwrite_submission_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mridge_test_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'second_ridge.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/scipy/sparse/construct.pyc\u001b[0m in \u001b[0;36mhstack\u001b[0;34m(blocks, format, dtype)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m     \"\"\"\n\u001b[0;32m--> 464\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/scipy/sparse/construct.pyc\u001b[0m in \u001b[0;36mbmat\u001b[0;34m(blocks, format, dtype)\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0mall_dtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblock_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mall_dtypes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mall_dtypes\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m     \u001b[0mrow_offsets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbrow_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/scipy/sparse/sputils.pyc\u001b[0m in \u001b[0;36mupcast\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'no supported conversion for types: %r'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: no supported conversion for types: (dtype('int64'), dtype('O'), dtype('int64'))"
     ]
    }
   ],
   "source": [
    "data_train = csr_matrix(hstack([other_data_train_title, X_train]))\n",
    "data_test = csr_matrix(hstack([other_data_test_title, X_test]))\n",
    "ridge.fit(data_train,y_train)\n",
    "ridge_test_pred = ridge.predict(data_test)\n",
    "write_submission_file(prediction=ridge_test_pred, filename='second_ridge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[u'medium.com'],\n",
       "       [u'medium.com'],\n",
       "       [u'medium.com'],\n",
       "       ..., \n",
       "       [u'medium.com'],\n",
       "       [u'medium.com'],\n",
       "       [u'byrslf.co']], dtype=object)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_data_train['published_year_month'].values.reshape(-1,1)\n",
    "other_data_train['domain'].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
